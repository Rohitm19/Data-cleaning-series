{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Introduction\n",
    "Data parsing and text data cleaning are essential techniques when working with textual data. Text data often contains inconsistencies, unnecessary information, or requires extraction of specific parts to make it useful for analysis.\n",
    "\n",
    "4.2 Techniques\n",
    "In this chapter, we'll cover several techniques:\n",
    "\n",
    "1. Extracting Meaningful Components: Extract specific information from text data, such as dates, names, or numbers.\n",
    "2. Ckeaning Text Data: Removing special characters, Converting text to lowercase, Removing extra spaces\n",
    "3. Removing Stop Words: Remove common words that do not add significant meaning to the text, such as \"and,\" \"the,\" or \"is.\"\n",
    "4. Lemmatization: Convert words to their base or dictionary form.\n",
    "5. Handling Contractions: Expand contracted words (e.g., \"don't\" to \"do not\").\n",
    "6. Removing HTML Tags: Clean text data from HTML tags if scraping from web pages.\n",
    "7. Removing Numerical Data: Remove numbers from text when they are not needed for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1 Extracting Meaningful Components\n",
    "\n",
    "Introduction:\n",
    "Extracting meaningful components from text data is often the first step in text data processing. This process involves isolating specific information within text, such as dates, phone numbers, email addresses, or any other relevant patterns. These components can be crucial for analysis, reporting, or further data processing.\n",
    "\n",
    "Task:\n",
    "Let's start with extracting dates from a 'Description' column in a dataset. We'll use regular expressions to identify and extract any date formats within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n",
      "\n",
      "DataFrame After Extracting Dates:\n",
      " Product ID Product Name  Price    Category  Stock              Description Dates\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget    []\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN    []\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish    []\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget    []\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient    []\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget    []\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget    []\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality    []\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features    []\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class    []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('D:/Projects/Data-cleaning-series/Chapter04 Data Parsing and Text Data Cleaning/Products.csv')\n",
    "\n",
    "# Example of the dataset with text data\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Extracting Dates from 'Description' column\n",
    "df['Dates'] = df['Description'].apply(lambda x: re.findall(r'\\d{4}-\\d{2}-\\d{2}', x) if pd.notnull(x) else [])\n",
    "\n",
    "# Display the DataFrame after extracting dates\n",
    "print(\"\\nDataFrame After Extracting Dates:\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Regular Expressions (re): We're using the re library to search for patterns in the text. The pattern \\d{4}-\\d{2}-\\d{2} is looking for dates in the format YYYY-MM-DD.\n",
    "\n",
    "Lambda Function: We apply a lambda function to the 'Description' column, searching for any text that matches the date pattern. If found, it adds it to a new column 'Dates'.\n",
    "\n",
    "Handling Missing Values: If the 'Description' is missing, the function returns an empty list to handle the missing data gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2 Cleaning Text Data\n",
    "\n",
    "Introduction:\n",
    "Cleaning text data is a crucial step in preparing data for analysis or machine learning models. It involves removing or correcting unwanted characters, formatting inconsistencies, and noise from the text. This process helps ensure that the data is in a consistent and usable format.\n",
    "\n",
    "Task:\n",
    "We'll clean the 'Description' column in our dataset by performing the following tasks:\n",
    "\n",
    "Removing special characters\n",
    "Converting text to lowercase\n",
    "Removing extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n",
      "\n",
      "DataFrame After Cleaning Text Data:\n",
      " Product ID Product Name  Price    Category  Stock              Description      Cleaned_Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget     a highquality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish      durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget       a versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient    compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget      multipurpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality          premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features        advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class            best in class\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('D:/Projects/Data-cleaning-series/Chapter04 Data Parsing and Text Data Cleaning/Products.csv')\n",
    "\n",
    "# Example of the dataset with text data\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'Description' column\n",
    "df['Cleaned_Description'] = df['Description'].apply(clean_text)\n",
    "\n",
    "# Display the DataFrame after cleaning text data\n",
    "print(\"\\nDataFrame After Cleaning Text Data:\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Removing Special Characters: The regular expression r'[^A-Za-z0-9\\s]' is used to remove any characters that are not letters, numbers, or spaces.\n",
    "\n",
    "Converting to Lowercase: We convert the text to lowercase to ensure uniformity in the data.\n",
    "\n",
    "Removing Extra Spaces: The regular expression r'\\s+' is used to replace multiple spaces with a single space, and strip() removes any leading or trailing spaces.\n",
    "\n",
    "This process cleans the text data, making it more consistent and ready for further processing or analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3. Removing Stop Words\n",
    "\n",
    "Introduction:\n",
    "Stop words are common words that usually carry little meaningful information for text analysis, such as \"and,\" \"the,\" \"is,\" etc. Removing stop words can help focus on the more important words in the text and improve the quality of text analysis and modeling.\n",
    "\n",
    "Task:\n",
    "We'll remove stop words from the 'Description' column in our dataset using the Natural Language Toolkit (nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame After Cleaning Text Data:\n",
      "             Description      Cleaned_Description\n",
      "   A high-quality widget     a highquality widget\n",
      "                     NaN                      NaN\n",
      "     Durable and stylish      durable and stylish\n",
      "      A versatile widget       a versatile widget\n",
      "   Compact and efficient    compact and efficient\n",
      "Latest technology widget latest technology widget\n",
      "    Multi-purpose widget      multipurpose widget\n",
      "         Premium quality          premium quality\n",
      "       Advanced features        advanced features\n",
      "           Best in class            best in class\n",
      "\n",
      "DataFrame After Removing Stop Words:\n",
      "     Cleaned_Description Description_No_Stop_Words\n",
      "    a highquality widget        highquality widget\n",
      "                     NaN                       NaN\n",
      "     durable and stylish           durable stylish\n",
      "      a versatile widget          versatile widget\n",
      "   compact and efficient         compact efficient\n",
      "latest technology widget  latest technology widget\n",
      "     multipurpose widget       multipurpose widget\n",
      "         premium quality           premium quality\n",
      "       advanced features         advanced features\n",
      "           best in class                best class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('D:/Projects/Data-cleaning-series/Chapter04 Data Parsing and Text Data Cleaning/Products.csv')\n",
    "\n",
    "# Download stopwords if not already present\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'Description' column\n",
    "df['Cleaned_Description'] = df['Description'].apply(clean_text)\n",
    "\n",
    "# Verify the DataFrame with the new column\n",
    "print(\"DataFrame After Cleaning Text Data:\")\n",
    "print(df[['Description', 'Cleaned_Description']].to_string(index=False))\n",
    "\n",
    "# Get the set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    if pd.isnull(text):\n",
    "        return text\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the remove_stop_words function to the 'Cleaned_Description' column\n",
    "df['Description_No_Stop_Words'] = df['Cleaned_Description'].apply(remove_stop_words)\n",
    "\n",
    "# Display the DataFrame after removing stop words\n",
    "print(\"\\nDataFrame After Removing Stop Words:\")\n",
    "print(df[['Cleaned_Description', 'Description_No_Stop_Words']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Tokenization: word_tokenize is used to split the text into individual words.\n",
    "\n",
    "Stop Words Removal: We filter out any words that are in the set of stop words.\n",
    "\n",
    "Reconstruction: The remaining words are joined back into a single string.\n",
    "\n",
    "This technique helps in focusing on the significant terms by removing common words that do not contribute much to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.4 Stemming and Lemmatization\n",
    "Introduction:\n",
    "Stemming and lemmatization are techniques used to reduce words to their base or root form. Stemming typically involves cutting off derivations, while lemmatization involves reducing words to their base form using linguistic rules. These techniques help in normalizing text and improving the quality of text analysis.\n",
    "\n",
    "Task:\n",
    "We'll apply stemming and lemmatization to the 'Description' column in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame After Stemming and Lemmatization:\n",
      " Product ID Product Name  Price    Category  Stock              Description      Cleaned_Description Description_No_Stop_Words     Description_Stemmed   Description_Lemmatized\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget    A high-quality widget       high-quality widget        high-qual widget      high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN                                                                                                    \n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish      Durable and stylish           Durable stylish          durabl stylish          Durable stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget       A versatile widget          versatile widget         versatil widget         versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient    Compact and efficient         Compact efficient          compact effici        Compact efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget Latest technology widget  Latest technology widget latest technolog widget Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget     Multi-purpose widget      Multi-purpose widget     multi-purpos widget     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality          Premium quality           Premium quality         premium qualiti          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features        Advanced features         Advanced features           advanc featur         Advanced feature\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class            Best in class                Best class              best class               Best class\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('D:/Projects/Data-cleaning-series/Chapter04 Data Parsing and Text Data Cleaning/Products.csv')\n",
    "\n",
    "# Example of the dataset with text data\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Download necessary nltk resources if not already present\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create the 'Cleaned_Description' column: clean up the text\n",
    "df['Cleaned_Description'] = df['Description'].fillna('').str.strip()\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    if pd.isnull(text) or text == '':\n",
    "        return text\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the remove_stop_words function to the 'Cleaned_Description' column\n",
    "df['Description_No_Stop_Words'] = df['Cleaned_Description'].apply(remove_stop_words)\n",
    "\n",
    "# Function for stemming\n",
    "def apply_stemming(text):\n",
    "    if pd.isnull(text) or text == '':\n",
    "        return text\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Function for lemmatization\n",
    "def apply_lemmatization(text):\n",
    "    if pd.isnull(text) or text == '':\n",
    "        return text\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply stemming and lemmatization to the 'Description_No_Stop_Words' column\n",
    "df['Description_Stemmed'] = df['Description_No_Stop_Words'].apply(apply_stemming)\n",
    "df['Description_Lemmatized'] = df['Description_No_Stop_Words'].apply(apply_lemmatization)\n",
    "\n",
    "# Display the DataFrame after stemming and lemmatization\n",
    "print(\"\\nDataFrame After Stemming and Lemmatization:\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Creating Cleaned_Description: We fill any missing values in the Description column with empty strings and strip any leading or trailing whitespace.\n",
    "\n",
    "Removing Stop Words: We apply the remove_stop_words function to the Cleaned_Description column.\n",
    "\n",
    "Applying Stemming and Lemmatization: We process the text in Description_No_Stop_Words to generate stemmed and lemmatized versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.5. Handling Dates and Times\n",
    "\n",
    "Introduction:\n",
    "Handling dates and times involves parsing, formatting, and extracting useful information from date and time fields. This is essential for analyzing time-based data, performing time series analysis, and ensuring consistency in date-time formats.\n",
    "\n",
    "Task:\n",
    "We'll parse dates from the 'Description' column, convert them to a standard datetime format, and extract features such as year, month, and day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n",
      "\n",
      "DataFrame After Extracting Date Features:\n",
      " Product ID Product Name  Price    Category  Stock              Description Year Month  Day\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget None  None None\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN None  None None\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish None  None None\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget None  None None\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient None  None None\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget None  None None\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget None  None None\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality None  None None\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features None  None None\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class None  None None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('D:/Projects/Data-cleaning-series/Chapter04 Data Parsing and Text Data Cleaning/Products.csv')\n",
    "\n",
    "# Example of the dataset with text data\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Function to parse and extract date features\n",
    "def parse_dates(text):\n",
    "    if pd.isnull(text):\n",
    "        return pd.Series([None, None, None])  # Year, Month, Day\n",
    "    try:\n",
    "        # Assuming the text might contain dates in 'YYYY-MM-DD' format\n",
    "        date = datetime.strptime(text, '%Y-%m-%d')\n",
    "        return pd.Series([date.year, date.month, date.day])\n",
    "    except ValueError:\n",
    "        return pd.Series([None, None, None])  # In case of parsing errors\n",
    "\n",
    "# Apply the function to extract date features\n",
    "df[['Year', 'Month', 'Day']] = df['Description'].apply(parse_dates)\n",
    "\n",
    "# Display the DataFrame after extracting date features\n",
    "print(\"\\nDataFrame After Extracting Date Features:\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Date Parsing: The datetime.strptime function converts text into a datetime object. We assume the date is in the 'YYYY-MM-DD' format.\n",
    "\n",
    "Extract Features: We extract year, month, and day from the datetime object and create new columns for each.\n",
    "\n",
    "Handling Parsing Errors: If the text cannot be parsed into a date, we return None values for the date features.\n",
    "\n",
    "This technique helps standardize and extract useful information from date and time fields, making it easier to perform time-based analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
