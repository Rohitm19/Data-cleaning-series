{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Introduction to Dealing with Duplicate Data\n",
    "\n",
    "In the world of data management, one of the most common and challenging issues is dealing with duplicate data. Duplicate data can arise from a variety of sources: merging datasets from different systems, data entry errors, or repeated data collection processes. While duplicates may seem harmless at first glance, they can lead to significant issues in data analysis, reporting, and decision-making, including inflated metrics, skewed results, and increased storage costs. Dealing with Duplicate Data is a critical step in the data cleaning process, ensuring that each record in the dataset is unique and accurately represents the information it contains.\n",
    "\n",
    "Definition\n",
    "\n",
    "Duplicate Data refers to instances where the same data point or record appears more than once in a dataset. This can include exact duplicates, where all fields match perfectly, or partial duplicates, where some fields match but others differ due to minor variations or errors.\n",
    "\n",
    "Dealing with Duplicate Data involves identifying these duplicate records and deciding on an appropriate method for handling them, whether by removing them, merging them, or flagging them for further review. This process is essential for maintaining the integrity and reliability of the dataset.\n",
    "\n",
    "Objective\n",
    "\n",
    "The primary objective of dealing with duplicate data is to ensure that the dataset is free from redundancy, which can distort analysis results and lead to incorrect conclusions. By identifying and removing duplicates, the goal is to create a dataset where each entry is unique, accurately reflecting the data's true value. This process improves the quality of the data, making it more reliable for analysis, reporting, and decision-making.\n",
    "\n",
    "Importance\n",
    "Dealing with duplicate data is crucial for several reasons:\n",
    "1. Accuracy in Analysis: Duplicates can significantly skew analysis results, leading to incorrect insights and potentially costly business decisions. Removing duplicates ensures that the analysis reflects the true state of the data.\n",
    "2. Data Integrity: Ensuring that each record is unique helps maintain the integrity of the data, which is vital for any data-driven process.\n",
    "3. Efficiency: Duplicate records increase the size of the dataset, leading to higher storage costs and longer processing times. By eliminating duplicates, the dataset becomes leaner and more efficient to work with.\n",
    "4. Improved Decision-Making: Clean, duplicate-free data supports better decision-making by providing accurate and reliable information.\n",
    "5. Consistency Across Systems: In integrated systems where data is shared across multiple platforms, dealing with duplicates ensures consistency and avoids discrepancies in reports generated from different sources.\n",
    "\n",
    "5.2 Techniques for Handling Duplicate Data\n",
    "1. Identifying Duplicates:\n",
    "   Identifying duplicates involves detecting rows in the dataset that are exactly or nearly identical.\n",
    "\n",
    "2. Removing Duplicates:\n",
    "   Removing duplicates refers to the process of deleting repeated rows to clean the dataset.\n",
    "\n",
    "3. Handling Partial Duplicates:\n",
    "   This technique involves managing rows that have identical values in some columns but differ in others.\n",
    "\n",
    "4. Fuzzy Matching for Near Duplicates:\n",
    "   Fuzzy matching identifies rows that are not exactly identical but are similar enough to be considered duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.1 Identifying Duplicates\n",
    "\n",
    "Identifying duplicates is the first step in dealing with redundant data. It involves scanning the dataset to find rows that appear more than once. These duplicates can be exact copies or may only match in specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n",
      "\n",
      "Duplicate Rows in the DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [Product ID, Product Name, Price, Category, Stock, Description]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('D:/Projects/Data-cleaning-series/Chapter05 Dealing with Duplicate Data/Products.csv')\n",
    "\n",
    "# Example of the dataset with potential duplicates\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Identify duplicate rows\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(\"\\nDuplicate Rows in the DataFrame:\")\n",
    "print(duplicates.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The duplicated() method in pandas identifies rows that are duplicated in the DataFrame. By default, it marks all rows except the first occurrence as duplicates.\n",
    "\n",
    "The resulting Boolean Series is then used to filter the DataFrame, displaying all duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.2 Removing Duplicates\n",
    "\n",
    "Once duplicates are identified, the next step is to remove them from the dataset. This process ensures that each row in the dataset is unique, which is crucial for accurate data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame After Removing Duplicates:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Display the DataFrame after removing duplicates\n",
    "print(\"\\nDataFrame After Removing Duplicates:\")\n",
    "print(df_cleaned.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The drop_duplicates() method removes duplicate rows from the DataFrame, leaving only the first occurrence of each duplicate.\n",
    "This method helps in cleaning the dataset, ensuring no redundant rows remain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.3 Handling Partial Duplicates\n",
    "\n",
    "Partial duplicates occur when rows have identical values in specific columns but differ in others. Handling these requires identifying the critical columns where duplication matters and removing or managing these partial duplicates accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partial Duplicates in the DataFrame (based on 'Product Name' and 'Category'):\n",
      "Empty DataFrame\n",
      "Columns: [Product ID, Product Name, Price, Category, Stock, Description]\n",
      "Index: []\n",
      "\n",
      "DataFrame After Removing Partial Duplicates:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n"
     ]
    }
   ],
   "source": [
    "# Identify partial duplicates based on specific columns\n",
    "partial_duplicates = df[df.duplicated(subset=['Product Name', 'Category'])]\n",
    "\n",
    "# Display partial duplicates\n",
    "print(\"\\nPartial Duplicates in the DataFrame (based on 'Product Name' and 'Category'):\")\n",
    "print(partial_duplicates.to_string(index=False))\n",
    "\n",
    "# Remove partial duplicates based on specific columns\n",
    "df_cleaned_partial = df.drop_duplicates(subset=['Product Name', 'Category'])\n",
    "\n",
    "# Display the DataFrame after removing partial duplicates\n",
    "print(\"\\nDataFrame After Removing Partial Duplicates:\")\n",
    "print(df_cleaned_partial.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "By specifying a subset of columns in the duplicated() method, you can identify partial duplicates based on those columns.\n",
    "\n",
    "The drop_duplicates() method with the subset parameter removes these partial duplicates, ensuring the dataset is clean where it matters most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.4 Fuzzy Matching for Near Duplicates\n",
    "\n",
    "Near duplicates are records that are not exact matches but are close enough in value to be considered duplicates. Fuzzy matching helps identify such records, which may occur due to minor differences in data entry or formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Near Duplicates in the 'Product Name' column:\n",
      "Original: Widget A, Match: Widget B, Score: 87.5\n",
      "Original: Widget A, Match: Widget D, Score: 87.5\n",
      "Original: Widget B, Match: Widget A, Score: 87.5\n",
      "Original: Widget B, Match: Widget D, Score: 87.5\n",
      "Original: Widget D, Match: Widget A, Score: 87.5\n",
      "Original: Widget D, Match: Widget B, Score: 87.5\n",
      "Original: Widget E, Match: Widget A, Score: 87.5\n",
      "Original: Widget E, Match: Widget B, Score: 87.5\n",
      "Original: Widget F, Match: Widget A, Score: 87.5\n",
      "Original: Widget F, Match: Widget B, Score: 87.5\n",
      "Original: Widget G, Match: Widget A, Score: 87.5\n",
      "Original: Widget G, Match: Widget B, Score: 87.5\n",
      "Original: Widget H, Match: Widget A, Score: 87.5\n",
      "Original: Widget H, Match: Widget B, Score: 87.5\n",
      "Original: Widget I, Match: Widget A, Score: 87.5\n",
      "Original: Widget I, Match: Widget B, Score: 87.5\n",
      "Original: Widget J, Match: Widget A, Score: 87.5\n",
      "Original: Widget J, Match: Widget B, Score: 87.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('D:/Projects/Data-cleaning-series/Chapter05 Dealing with Duplicate Data/Products.csv')\n",
    "\n",
    "# Define a function to find near duplicates using fuzzy matching\n",
    "def find_near_duplicates(df, column_name, threshold=80, top_n=3):\n",
    "    # Get unique values from the specified column\n",
    "    unique_values = df[column_name].dropna().unique()\n",
    "    near_duplicates = []\n",
    "    \n",
    "    for value in unique_values:\n",
    "        # Extract matches with the specified threshold and limit\n",
    "        matches = process.extract(value, unique_values, scorer=fuzz.ratio, limit=top_n, score_cutoff=threshold)\n",
    "        for match in matches:\n",
    "            if match[0] != value:\n",
    "                near_duplicates.append((value, match[0], match[1]))\n",
    "    \n",
    "    return near_duplicates\n",
    "\n",
    "# Find near duplicates in the 'Product Name' column\n",
    "near_duplicates = find_near_duplicates(df, 'Product Name')\n",
    "\n",
    "# Display near duplicates\n",
    "print(\"\\nNear Duplicates in the 'Product Name' column:\")\n",
    "if near_duplicates:\n",
    "    for original, match, score in near_duplicates:\n",
    "        print(f\"Original: {original}, Match: {match}, Score: {score}\")\n",
    "else:\n",
    "    print(\"No near duplicates found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Threshold Adjustment: The threshold is set to 80 to capture more potential matches.\n",
    "\n",
    "Conditional Output: The code checks if any near duplicates are found before printing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
