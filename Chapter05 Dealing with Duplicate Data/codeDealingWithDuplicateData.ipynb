{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Dealing with Duplicate Data\n",
    "\n",
    "Introduction:\n",
    "Duplicate data is a common issue in datasets that can lead to skewed analysis and inaccurate insights. It occurs when identical or near-identical rows exist in a dataset, often as a result of data entry errors, merging datasets, or automated data collection processes. Addressing duplicates is essential to ensure the quality and reliability of your data.\n",
    "\n",
    "Definition:\n",
    "Duplicate data refers to instances in which two or more identical or nearly identical rows appear in a dataset. These duplicates can include exact matches across all columns or partial matches in specific columns.\n",
    "\n",
    "Objective:\n",
    "The primary objective of dealing with duplicate data is to identify, analyze, and eliminate any redundant information in the dataset. This process ensures that the data used in analysis or modeling is accurate and free from unnecessary repetition.\n",
    "\n",
    "Importance:\n",
    "Handling duplicate data is crucial because duplicates can distort statistical analysis, lead to incorrect conclusions, and reduce the efficiency of data processing. Removing or correctly handling duplicates helps maintain data integrity, leading to more reliable and meaningful insights.\n",
    "\n",
    "5.2 Techniques for Handling Duplicate Data\n",
    "1. Identifying Duplicates:\n",
    "   Identifying duplicates involves detecting rows in the dataset that are exactly or nearly identical.\n",
    "\n",
    "2. Removing Duplicates:\n",
    "   Removing duplicates refers to the process of deleting repeated rows to clean the dataset.\n",
    "\n",
    "3. Handling Partial Duplicates:\n",
    "   This technique involves managing rows that have identical values in some columns but differ in others.\n",
    "\n",
    "4. Fuzzy Matching for Near Duplicates:\n",
    "   Fuzzy matching identifies rows that are not exactly identical but are similar enough to be considered duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.1 Identifying Duplicates\n",
    "\n",
    "Introduction:\n",
    "Identifying duplicates is the first step in dealing with redundant data. It involves scanning the dataset to find rows that appear more than once. These duplicates can be exact copies or may only match in specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n",
      "\n",
      "Duplicate Rows in the DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [Product ID, Product Name, Price, Category, Stock, Description]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('D:/Projects/Data-cleaning-series/Chapter05 Dealing with Duplicate Data/Products.csv')\n",
    "\n",
    "# Example of the dataset with potential duplicates\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Identify duplicate rows\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "# Display duplicate rows\n",
    "print(\"\\nDuplicate Rows in the DataFrame:\")\n",
    "print(duplicates.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The duplicated() method in pandas identifies rows that are duplicated in the DataFrame. By default, it marks all rows except the first occurrence as duplicates.\n",
    "\n",
    "The resulting Boolean Series is then used to filter the DataFrame, displaying all duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.2 Technique 2: Removing Duplicates\n",
    "\n",
    "Introduction:\n",
    "Once duplicates are identified, the next step is to remove them from the dataset. This process ensures that each row in the dataset is unique, which is crucial for accurate data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame After Removing Duplicates:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Display the DataFrame after removing duplicates\n",
    "print(\"\\nDataFrame After Removing Duplicates:\")\n",
    "print(df_cleaned.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "The drop_duplicates() method removes duplicate rows from the DataFrame, leaving only the first occurrence of each duplicate.\n",
    "This method helps in cleaning the dataset, ensuring no redundant rows remain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.3 Handling Partial Duplicates\n",
    "\n",
    "Introduction:\n",
    "Partial duplicates occur when rows have identical values in specific columns but differ in others. Handling these requires identifying the critical columns where duplication matters and removing or managing these partial duplicates accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partial Duplicates in the DataFrame (based on 'Product Name' and 'Category'):\n",
      "Empty DataFrame\n",
      "Columns: [Product ID, Product Name, Price, Category, Stock, Description]\n",
      "Index: []\n",
      "\n",
      "DataFrame After Removing Partial Duplicates:\n",
      " Product ID Product Name  Price    Category  Stock              Description\n",
      "          1     Widget A  19.99 Electronics  100.0    A high-quality widget\n",
      "          2     Widget B  29.99 Electronics    NaN                      NaN\n",
      "          3          NaN  15.00  Home Goods   50.0      Durable and stylish\n",
      "          4     Widget D    NaN  Home Goods  200.0       A versatile widget\n",
      "          5     Widget E   9.99         NaN   10.0    Compact and efficient\n",
      "          6     Widget F  25.00 Electronics    0.0 Latest technology widget\n",
      "          7     Widget G    NaN     Kitchen  150.0     Multi-purpose widget\n",
      "          8     Widget H  39.99     Kitchen   75.0          Premium quality\n",
      "          9     Widget I    NaN Electronics    NaN        Advanced features\n",
      "         10     Widget J  49.99 Electronics   60.0            Best in class\n"
     ]
    }
   ],
   "source": [
    "# Identify partial duplicates based on specific columns\n",
    "partial_duplicates = df[df.duplicated(subset=['Product Name', 'Category'])]\n",
    "\n",
    "# Display partial duplicates\n",
    "print(\"\\nPartial Duplicates in the DataFrame (based on 'Product Name' and 'Category'):\")\n",
    "print(partial_duplicates.to_string(index=False))\n",
    "\n",
    "# Remove partial duplicates based on specific columns\n",
    "df_cleaned_partial = df.drop_duplicates(subset=['Product Name', 'Category'])\n",
    "\n",
    "# Display the DataFrame after removing partial duplicates\n",
    "print(\"\\nDataFrame After Removing Partial Duplicates:\")\n",
    "print(df_cleaned_partial.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "By specifying a subset of columns in the duplicated() method, you can identify partial duplicates based on those columns.\n",
    "\n",
    "The drop_duplicates() method with the subset parameter removes these partial duplicates, ensuring the dataset is clean where it matters most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.4 Fuzzy Matching for Near Duplicates\n",
    "\n",
    "Introduction:\n",
    "Near duplicates are records that are not exact matches but are close enough in value to be considered duplicates. Fuzzy matching helps identify such records, which may occur due to minor differences in data entry or formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Near Duplicates in the 'Product Name' column:\n",
      "Original: Widget A, Match: Widget B, Score: 87.5\n",
      "Original: Widget A, Match: Widget D, Score: 87.5\n",
      "Original: Widget B, Match: Widget A, Score: 87.5\n",
      "Original: Widget B, Match: Widget D, Score: 87.5\n",
      "Original: Widget D, Match: Widget A, Score: 87.5\n",
      "Original: Widget D, Match: Widget B, Score: 87.5\n",
      "Original: Widget E, Match: Widget A, Score: 87.5\n",
      "Original: Widget E, Match: Widget B, Score: 87.5\n",
      "Original: Widget F, Match: Widget A, Score: 87.5\n",
      "Original: Widget F, Match: Widget B, Score: 87.5\n",
      "Original: Widget G, Match: Widget A, Score: 87.5\n",
      "Original: Widget G, Match: Widget B, Score: 87.5\n",
      "Original: Widget H, Match: Widget A, Score: 87.5\n",
      "Original: Widget H, Match: Widget B, Score: 87.5\n",
      "Original: Widget I, Match: Widget A, Score: 87.5\n",
      "Original: Widget I, Match: Widget B, Score: 87.5\n",
      "Original: Widget J, Match: Widget A, Score: 87.5\n",
      "Original: Widget J, Match: Widget B, Score: 87.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('D:/Projects/Data-cleaning-series/Chapter05 Dealing with Duplicate Data/Products.csv')\n",
    "\n",
    "# Define a function to find near duplicates using fuzzy matching\n",
    "def find_near_duplicates(df, column_name, threshold=80, top_n=3):\n",
    "    # Get unique values from the specified column\n",
    "    unique_values = df[column_name].dropna().unique()\n",
    "    near_duplicates = []\n",
    "    \n",
    "    for value in unique_values:\n",
    "        # Extract matches with the specified threshold and limit\n",
    "        matches = process.extract(value, unique_values, scorer=fuzz.ratio, limit=top_n, score_cutoff=threshold)\n",
    "        for match in matches:\n",
    "            if match[0] != value:\n",
    "                near_duplicates.append((value, match[0], match[1]))\n",
    "    \n",
    "    return near_duplicates\n",
    "\n",
    "# Find near duplicates in the 'Product Name' column\n",
    "near_duplicates = find_near_duplicates(df, 'Product Name')\n",
    "\n",
    "# Display near duplicates\n",
    "print(\"\\nNear Duplicates in the 'Product Name' column:\")\n",
    "if near_duplicates:\n",
    "    for original, match, score in near_duplicates:\n",
    "        print(f\"Original: {original}, Match: {match}, Score: {score}\")\n",
    "else:\n",
    "    print(\"No near duplicates found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Threshold Adjustment: The threshold is set to 80 to capture more potential matches.\n",
    "\n",
    "Conditional Output: The code checks if any near duplicates are found before printing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
