{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.1 Data Enrichment and Feature Engineering\n",
    "\n",
    "Introduction\n",
    "Data enrichment and feature engineering involve enhancing the quality and depth of data by adding new information or deriving new features from existing data. This process helps improve the performance of machine learning models by providing them with more meaningful inputs.\n",
    "\n",
    "Definition\n",
    "Data Enrichment: The process of augmenting existing data by adding new information from external sources or through calculated metrics.\n",
    "Feature Engineering: The practice of transforming raw data into features that better represent the underlying problem to the predictive models.\n",
    "\n",
    "Objective\n",
    "The main objective is to create more informative features that can capture the relationships and patterns within the data, ultimately leading to better model performance.\n",
    "\n",
    "Importance\n",
    "Feature engineering and data enrichment are critical for improving the accuracy and robustness of machine learning models. By generating relevant features, you can provide the model with the necessary inputs to make more informed predictions.\n",
    "\n",
    "10.2 Techniques List and Definitions\n",
    "1. Creating Interaction Features: Combining two or more features to capture the interaction between them.\n",
    "2. Adding External Data: Enriching the dataset with external sources, such as demographic information, weather data, or economic indicators.\n",
    "3. Aggregating Features: Summarizing data through aggregations like mean, sum, or count based on different categories or time periods.\n",
    "4. Creating Lag Features: Creating features based on previous time steps in time series data.\n",
    "5. Calculating Rolling Statistics: Using moving averages, sums, or other statistics calculated over a rolling window.\n",
    "6. Binning Continuous Variables: Converting continuous variables into categorical bins to capture non-linear relationships.\n",
    "7. Polynomial Feature Creation: Generating polynomial features to capture non-linear relationships between variables.\n",
    "8. Text Feature Extraction: Extracting meaningful features from text data, such as word counts or sentiment scores.\n",
    "9. Encoding Cyclical Features: Encoding cyclical features, such as time or geographic data, to capture periodicity.\n",
    "10. Dimensionality Reduction Techniques: Using techniques like PCA to reduce the number of features while retaining essential information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.1 Creating Interaction Features\n",
    "\n",
    "Introduction\n",
    "Interaction features capture the combined effect of two or more features, which may not be apparent when considering the features individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2  Feature1_Feature2_Interaction\n",
      "0        10         1                             10\n",
      "1        20         2                             40\n",
      "2        30         3                             90\n",
      "3        40         4                            160\n",
      "4        50         5                            250\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Feature1': [10, 20, 30, 40, 50],\n",
    "    'Feature2': [1, 2, 3, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating Interaction Feature\n",
    "df['Feature1_Feature2_Interaction'] = df['Feature1'] * df['Feature2']\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Interaction Feature: The new feature, Feature1_Feature2_Interaction, is created by multiplying Feature1 and Feature2.\n",
    "Purpose: This feature may capture interactions that are important for predicting the target variable, improving model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.2 Adding External Data\n",
    "\n",
    "Introduction\n",
    "Adding external data can enrich the existing dataset with additional context, leading to better predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Product ID  Sales  Average_Temperature\n",
      "0           1    100                   30\n",
      "1           2    150                   25\n",
      "2           3    200                   27\n",
      "3           4    130                   32\n",
      "4           5    170                   28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Product ID': [1, 2, 3, 4, 5],\n",
    "    'Sales': [100, 150, 200, 130, 170]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# External Data - Adding weather information\n",
    "external_data = {\n",
    "    'Product ID': [1, 2, 3, 4, 5],\n",
    "    'Average_Temperature': [30, 25, 27, 32, 28]\n",
    "}\n",
    "external_df = pd.DataFrame(external_data)\n",
    "\n",
    "# Merging External Data\n",
    "df = df.merge(external_df, on='Product ID', how='left')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "External Data: The dataset is enriched with weather information, which might be relevant for predicting sales.\n",
    "Merging: The merge function is used to combine the existing dataset with the external data based on a common key (Product ID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.3 Aggregating Features\n",
    "\n",
    "Introduction\n",
    "Aggregating features involves summarizing data through various statistical measures, such as mean, sum, or count, often based on groupings or time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Customer ID': [1, 1, 2, 2, 3],\n",
    "    'Purchase Amount': [100, 200, 150, 250, 300]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Aggregating Purchase Amount by Customer ID\n",
    "df_aggregated = df.groupby('Customer ID')['Purchase Amount'].sum().reset_index()\n",
    "\n",
    "print(df_aggregated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Aggregation: The groupby function is used to sum the Purchase Amount for each Customer ID, providing insight into total spending by each customer.\n",
    "Purpose: Aggregated features can simplify the dataset and reveal patterns or trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.4 Creating Lag Features\n",
    "\n",
    "Introduction\n",
    "Lag features involve using past values of a variable as predictors in time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Sales  Sales_Lag1\n",
      "0 2023-01-01    100         NaN\n",
      "1 2023-01-02    150       100.0\n",
      "2 2023-01-03    130       150.0\n",
      "3 2023-01-04    170       130.0\n",
      "4 2023-01-05    160       170.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Date': pd.date_range(start='2023-01-01', periods=5, freq='D'),\n",
    "    'Sales': [100, 150, 130, 170, 160]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating Lag Feature\n",
    "df['Sales_Lag1'] = df['Sales'].shift(1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Lag Feature: The new feature Sales_Lag1 contains the sales values from the previous day.\n",
    "Purpose: Lag features are essential in time series analysis to capture temporal dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.5 Calculating Rolling Statistics\n",
    "\n",
    "Introduction\n",
    "Rolling statistics are computed over a specified window of past observations, smoothing out short-term fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Sales  Rolling_Mean\n",
      "0 2023-01-01    100           NaN\n",
      "1 2023-01-02    150           NaN\n",
      "2 2023-01-03    130    126.666667\n",
      "3 2023-01-04    170    150.000000\n",
      "4 2023-01-05    160    153.333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Date': pd.date_range(start='2023-01-01', periods=5, freq='D'),\n",
    "    'Sales': [100, 150, 130, 170, 160]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculating Rolling Mean\n",
    "df['Rolling_Mean'] = df['Sales'].rolling(window=3).mean()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Rolling Mean: This feature calculates the average sales over a 3-day window.\n",
    "Purpose: Rolling statistics help smooth out noise in time series data and reveal underlying trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.6 Binning Continuous Variables\n",
    "\n",
    "Introduction\n",
    "Binning involves converting continuous variables into discrete categories or bins, which can help in capturing non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {'Age': [22, 35, 58, 45, 29]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Binning Age into categories\n",
    "bins = [0, 25, 45, 60]\n",
    "labels = ['Youth', 'Adult', 'Senior']\n",
    "df['Age Group'] = pd.cut(df['Age'], bins=bins, labels=labels)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Binning: The pd.cut function is used to categorize ages into three groups: 'Youth', 'Adult', and 'Senior'.\n",
    "Purpose: Binning helps in reducing the impact of outliers and capturing non-linear effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.7. Polynomial Feature Creation\n",
    "\n",
    "Introduction\n",
    "Polynomial features are created by raising existing features to a power, which allows the model to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature1_squared\n",
      "0       1.0               1.0\n",
      "1       2.0               4.0\n",
      "2       3.0               9.0\n",
      "3       4.0              16.0\n",
      "4       5.0              25.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Sample Data\n",
    "data = {'Feature1': [1, 2, 3, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(df[['Feature1']])\n",
    "\n",
    "df_poly = pd.DataFrame(poly_features, columns=['Feature1', 'Feature1_squared'])\n",
    "print(df_poly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Polynomial Features: The original feature Feature1 is squared to create a new feature Feature1_squared.\n",
    "Purpose: Polynomial features enable the model to fit more complex, non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.8. Text Feature Extraction\n",
    "\n",
    "Introduction\n",
    "Text feature extraction involves converting unstructured text data into meaningful numerical features for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample Data\n",
    "data = {'Text': ['I love data science', 'Data science is great', 'I love learning new things']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Text Feature Extraction using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "text_features = vectorizer.fit_transform(df['Text']).toarray()\n",
    "\n",
    "df_text_features = pd.DataFrame(text_features, columns=vectorizer.get_feature_names_out())\n",
    "print(df_text_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "CountVectorizer: This technique converts text data into a matrix of token counts, creating features based on word frequency.\n",
    "Purpose: Extracted features from text data are used in various machine learning models to analyze textual information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
