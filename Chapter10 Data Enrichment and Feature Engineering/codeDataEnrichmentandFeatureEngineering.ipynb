{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.1 Data Enrichment and Feature Engineering\n",
    "\n",
    "Data enrichment and feature engineering are crucial steps in the data analysis pipeline, aimed at enhancing the quality and predictive power of a dataset. These processes involve adding additional information to the dataset, creating new variables (features), and transforming existing ones to better capture the underlying patterns in the data. Data enrichment brings in external or supplementary data, while feature engineering transforms this and existing data into a format that can improve the performance of predictive models and analysis.\n",
    "\n",
    "In todayâ€™s data-driven world, the raw data collected from various sources often lacks the depth or context needed for comprehensive analysis. By enriching data and engineering features, analysts can uncover hidden relationships, enhance model accuracy, and derive more meaningful insights. This chapter delves into the strategies and techniques for effectively enriching data and engineering features to maximize their value in the analytical process.\n",
    "\n",
    "Definition\n",
    "\n",
    "Data Enrichment: The process of enhancing existing data by incorporating additional information from external or supplementary sources. This can involve appending new columns with related data, such as demographics, geographic information, or transactional history, to provide more context and depth.\n",
    "\n",
    "Feature Engineering: The practice of creating new features (variables) or transforming existing ones to improve the predictive power of models. This can include deriving new features from raw data, encoding categorical variables, creating interaction terms, or applying mathematical transformations.\n",
    "\n",
    "Objective\n",
    "\n",
    "The primary objectives of data enrichment and feature engineering are:\n",
    "\n",
    "1. Enhancing Data Quality: By enriching data, we add valuable context that enhances the quality and completeness of the dataset.\n",
    "2. Improving Predictive Models: Feature engineering helps in creating more informative features that can improve the performance of machine learning models.\n",
    "3. Uncovering Hidden Patterns: These processes help in identifying and leveraging hidden patterns within the data that might not be apparent from the raw data.\n",
    "4. Increasing Model Interpretability: Carefully engineered features can make models more interpretable, as they often align more closely with the problem domain.\n",
    "\n",
    "Importance\n",
    "\n",
    "Data enrichment and feature engineering are vital for several reasons:\n",
    "\n",
    "1. Boosting Model Accuracy: Well-engineered features can significantly improve the accuracy and robustness of predictive models.\n",
    "2. Providing Contextual Insights: Enriched data offers additional context, leading to more nuanced and actionable insights.\n",
    "3. Addressing Data Gaps: Enrichment helps in filling in the gaps within the existing dataset, leading to a more comprehensive analysis.\n",
    "4. Enabling Complex Analysis: Feature engineering allows for the transformation of raw data into formats that can be used in complex analyses, such as machine learning or statistical modeling.\n",
    "\n",
    "10.2 Techniques List and Definitions\n",
    "1. Creating Interaction Features: Combining two or more features to capture the interaction between them.\n",
    "2. Adding External Data: Enriching the dataset with external sources, such as demographic information, weather data, or economic indicators.\n",
    "3. Aggregating Features: Summarizing data through aggregations like mean, sum, or count based on different categories or time periods.\n",
    "4. Creating Lag Features: Creating features based on previous time steps in time series data.\n",
    "5. Calculating Rolling Statistics: Using moving averages, sums, or other statistics calculated over a rolling window.\n",
    "6. Binning Continuous Variables: Converting continuous variables into categorical bins to capture non-linear relationships.\n",
    "7. Polynomial Feature Creation: Generating polynomial features to capture non-linear relationships between variables.\n",
    "8. Text Feature Extraction: Extracting meaningful features from text data, such as word counts or sentiment scores.\n",
    "9. Encoding Cyclical Features: Encoding cyclical features, such as time or geographic data, to capture periodicity.\n",
    "10. Dimensionality Reduction Techniques: Using techniques like PCA to reduce the number of features while retaining essential information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.1 Creating Interaction Features\n",
    "\n",
    "Interaction features capture the combined effect of two or more features, which may not be apparent when considering the features individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2  Feature1_Feature2_Interaction\n",
      "0        10         1                             10\n",
      "1        20         2                             40\n",
      "2        30         3                             90\n",
      "3        40         4                            160\n",
      "4        50         5                            250\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Feature1': [10, 20, 30, 40, 50],\n",
    "    'Feature2': [1, 2, 3, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating Interaction Feature\n",
    "df['Feature1_Feature2_Interaction'] = df['Feature1'] * df['Feature2']\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Interaction Feature: The new feature, Feature1_Feature2_Interaction, is created by multiplying Feature1 and Feature2.\n",
    "Purpose: This feature may capture interactions that are important for predicting the target variable, improving model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.2 Adding External Data\n",
    "\n",
    "Adding external data can enrich the existing dataset with additional context, leading to better predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Product ID  Sales  Average_Temperature\n",
      "0           1    100                   30\n",
      "1           2    150                   25\n",
      "2           3    200                   27\n",
      "3           4    130                   32\n",
      "4           5    170                   28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Product ID': [1, 2, 3, 4, 5],\n",
    "    'Sales': [100, 150, 200, 130, 170]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# External Data - Adding weather information\n",
    "external_data = {\n",
    "    'Product ID': [1, 2, 3, 4, 5],\n",
    "    'Average_Temperature': [30, 25, 27, 32, 28]\n",
    "}\n",
    "external_df = pd.DataFrame(external_data)\n",
    "\n",
    "# Merging External Data\n",
    "df = df.merge(external_df, on='Product ID', how='left')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "External Data: The dataset is enriched with weather information, which might be relevant for predicting sales.\n",
    "Merging: The merge function is used to combine the existing dataset with the external data based on a common key (Product ID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.3 Aggregating Features\n",
    "\n",
    "Aggregating features involves summarizing data through various statistical measures, such as mean, sum, or count, often based on groupings or time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Customer ID  Purchase Amount\n",
      "0            1              300\n",
      "1            2              400\n",
      "2            3              300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Customer ID': [1, 1, 2, 2, 3],\n",
    "    'Purchase Amount': [100, 200, 150, 250, 300]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Aggregating Purchase Amount by Customer ID\n",
    "df_aggregated = df.groupby('Customer ID')['Purchase Amount'].sum().reset_index()\n",
    "\n",
    "print(df_aggregated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Aggregation: The groupby function is used to sum the Purchase Amount for each Customer ID, providing insight into total spending by each customer.\n",
    "Purpose: Aggregated features can simplify the dataset and reveal patterns or trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.4 Creating Lag Features\n",
    "\n",
    "Lag features involve using past values of a variable as predictors in time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Sales  Sales_Lag1\n",
      "0 2023-01-01    100         NaN\n",
      "1 2023-01-02    150       100.0\n",
      "2 2023-01-03    130       150.0\n",
      "3 2023-01-04    170       130.0\n",
      "4 2023-01-05    160       170.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Date': pd.date_range(start='2023-01-01', periods=5, freq='D'),\n",
    "    'Sales': [100, 150, 130, 170, 160]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating Lag Feature\n",
    "df['Sales_Lag1'] = df['Sales'].shift(1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Lag Feature: The new feature Sales_Lag1 contains the sales values from the previous day.\n",
    "Purpose: Lag features are essential in time series analysis to capture temporal dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.5 Calculating Rolling Statistics\n",
    "\n",
    "Rolling statistics are computed over a specified window of past observations, smoothing out short-term fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Sales  Rolling_Mean\n",
      "0 2023-01-01    100           NaN\n",
      "1 2023-01-02    150           NaN\n",
      "2 2023-01-03    130    126.666667\n",
      "3 2023-01-04    170    150.000000\n",
      "4 2023-01-05    160    153.333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Date': pd.date_range(start='2023-01-01', periods=5, freq='D'),\n",
    "    'Sales': [100, 150, 130, 170, 160]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculating Rolling Mean\n",
    "df['Rolling_Mean'] = df['Sales'].rolling(window=3).mean()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Rolling Mean: This feature calculates the average sales over a 3-day window.\n",
    "Purpose: Rolling statistics help smooth out noise in time series data and reveal underlying trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.6 Binning Continuous Variables\n",
    "\n",
    "Binning involves converting continuous variables into discrete categories or bins, which can help in capturing non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age Age Group\n",
      "0   22     Youth\n",
      "1   35     Adult\n",
      "2   58    Senior\n",
      "3   45     Adult\n",
      "4   29     Adult\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample Data\n",
    "data = {'Age': [22, 35, 58, 45, 29]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Binning Age into categories\n",
    "bins = [0, 25, 45, 60]\n",
    "labels = ['Youth', 'Adult', 'Senior']\n",
    "df['Age Group'] = pd.cut(df['Age'], bins=bins, labels=labels)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Binning: The pd.cut function is used to categorize ages into three groups: 'Youth', 'Adult', and 'Senior'.\n",
    "Purpose: Binning helps in reducing the impact of outliers and capturing non-linear effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.7. Polynomial Feature Creation\n",
    "\n",
    "Polynomial features are created by raising existing features to a power, which allows the model to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature1_squared\n",
      "0       1.0               1.0\n",
      "1       2.0               4.0\n",
      "2       3.0               9.0\n",
      "3       4.0              16.0\n",
      "4       5.0              25.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Sample Data\n",
    "data = {'Feature1': [1, 2, 3, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(df[['Feature1']])\n",
    "\n",
    "df_poly = pd.DataFrame(poly_features, columns=['Feature1', 'Feature1_squared'])\n",
    "print(df_poly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Polynomial Features: The original feature Feature1 is squared to create a new feature Feature1_squared.\n",
    "Purpose: Polynomial features enable the model to fit more complex, non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.8. Text Feature Extraction\n",
    "\n",
    "Text feature extraction involves converting unstructured text data into meaningful numerical features for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   data  great  is  learning  love  new  science  things\n",
      "0     1      0   0         0     1    0        1       0\n",
      "1     1      1   1         0     0    0        1       0\n",
      "2     0      0   0         1     1    1        0       1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample Data\n",
    "data = {'Text': ['I love data science', 'Data science is great', 'I love learning new things']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Text Feature Extraction using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "text_features = vectorizer.fit_transform(df['Text']).toarray()\n",
    "\n",
    "df_text_features = pd.DataFrame(text_features, columns=vectorizer.get_feature_names_out())\n",
    "print(df_text_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "CountVectorizer: This technique converts text data into a matrix of token counts, creating features based on word frequency.\n",
    "Purpose: Extracted features from text data are used in various machine learning models to analyze textual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.9. Encoding Cyclical Features\n",
    "\n",
    "Cyclical features represent variables with periodic patterns, such as time or geographic data, and are encoded to capture their cyclic nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hour      Hour_sin      Hour_cos\n",
      "0     0  0.000000e+00  1.000000e+00\n",
      "1     6  1.000000e+00  6.123234e-17\n",
      "2    12  1.224647e-16 -1.000000e+00\n",
      "3    18 -1.000000e+00 -1.836970e-16\n",
      "4    23 -2.588190e-01  9.659258e-01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample Data\n",
    "data = {'Hour': [0, 6, 12, 18, 23]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Encoding Hour as Cyclical Feature\n",
    "df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "Cyclical Encoding: The Hour feature is transformed into two new features using sine and cosine functions to represent its cyclical nature.\n",
    "Purpose: Encoding cyclical features helps capture patterns that repeat over time, improving model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2.10 Dimensionality Reduction Techniques\n",
    "\n",
    "Dimensionality reduction techniques reduce the number of features while preserving essential information, which helps in avoiding overfitting and improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PC1           PC2\n",
      "0  6.0  4.282044e-16\n",
      "1  3.0 -1.427348e-16\n",
      "2 -0.0 -0.000000e+00\n",
      "3 -3.0  1.427348e-16\n",
      "4 -6.0  2.854696e-16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5],\n",
    "    'Feature2': [2, 4, 6, 8, 10],\n",
    "    'Feature3': [5, 7, 9, 11, 13]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Applying PCA for Dimensionality Reduction\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(df)\n",
    "\n",
    "df_reduced = pd.DataFrame(reduced_features, columns=['PC1', 'PC2'])\n",
    "print(df_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "\n",
    "PCA: Principal Component Analysis (PCA) is used to reduce the dataset to two principal components (PC1 and PC2), which explain the maximum variance in the data.\n",
    "Purpose: Dimensionality reduction simplifies the dataset and helps improve model generalization by focusing on the most important features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
